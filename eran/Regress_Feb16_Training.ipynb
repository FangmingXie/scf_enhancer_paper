{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CNN regression to predict the mCG level of all DMRs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, re\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import timeit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kmers = True\n",
    "N1 = 2\n",
    "N2 = 2\n",
    "\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RegressData/Regress_Feb16_data_genes2enhu.pkl\n",
      "Loaded RegressData/Regress_Feb16_data_rnau.pkl\n",
      "Loaded RegressData/Regress_Feb16_data_df_mlevelu.pkl\n",
      "Loaded RegressData/Regress_Feb16_data_df_atacu.pkl\n",
      "Loaded RegressData/Regress_Feb16_data_genes.pkl\n"
     ]
    }
   ],
   "source": [
    "fn_save_prefix = 'RegressData/Regress_Feb16_data_'\n",
    "save_vars = ['genes2enhu', 'rnau', 'df_mlevelu', 'df_atacu', 'genes']\n",
    "for var in save_vars:\n",
    "    fn_save = fn_save_prefix+var+'.pkl'\n",
    "    cmd = '%s=pd.read_pickle(\"%s\")' % (var, fn_save)\n",
    "    exec(cmd)\n",
    "    print('Loaded %s' % fn_save)\n",
    "\n",
    "if use_kmers:\n",
    "    with np.load('RegressData/Regress_Feb16_data_kmer_countsu.npz', allow_pickle=True) as x:\n",
    "        kmer_countsu=x['kmer_countsu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cndd/emukamel/conda_envs/scanpy/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1116: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "# Impute the NaN values in mlevel\n",
    "df_mlevelu_mean = np.outer(df_mlevelu.median(axis=1).values, df_mlevelu.median(axis=0).values)\n",
    "df_mlevelu_mean = pd.DataFrame(df_mlevelu_mean, index=df_mlevelu.index, columns=df_mlevelu.columns)\n",
    "df_mlevelu_mean = df_mlevelu_mean.fillna(df_mlevelu.median())\n",
    "df_mlevelu = df_mlevelu.fillna(df_mlevelu_mean)\n",
    "assert(not df_mlevelu.isna().any(axis=None))\n",
    "assert(not df_atacu.isna().any(axis=None))\n",
    "ensids = rnau.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out bad enhancers\n",
    "bad_enh = df_mlevelu.loc[genes2enhu.loc[:,'enh_pos']].isna().any(axis=1)\n",
    "bad_enh = bad_enh | df_atacu.loc[genes2enhu.loc[:,'enh_pos']].isna().any(axis=1)\n",
    "good_enh = [not i for i in bad_enh]\n",
    "genes2enhu = genes2enhu[good_enh]\n",
    "if use_kmers:\n",
    "    kmer_countsu = kmer_countsu[good_enh]\n",
    "    K = kmer_countsu.shape[1] # Number of K-mers\n",
    "else:\n",
    "    K = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the mC and ATAC data into a numpy array for quick indexing\n",
    "# %%timeit\n",
    "max_nenh = genes2enhu.groupby('ensid')['gene_chr'].count().max()\n",
    "Ne = ensids.shape[0]\n",
    "Nc = df_mlevelu.shape[1]\n",
    "rna_lookup = rnau.loc[ensids].to_numpy()\n",
    "ml_lookup = np.zeros((Ne,max_nenh,Nc))\n",
    "atac_lookup = np.zeros((Ne,max_nenh,Nc))\n",
    "for i,ens in enumerate(ensids):\n",
    "    curr_enh = genes2enhu.loc[ens,['enh_pos','enh_num']]\n",
    "    mlu = df_mlevelu.loc[curr_enh['enh_pos']].to_numpy()\n",
    "    atacu = df_atacu.loc[curr_enh['enh_pos']]\n",
    "    ml_lookup[i,:mlu.shape[0],:] = mlu\n",
    "    atac_lookup[i,:atacu.shape[0],:] = atacu\n",
    "# ensids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3947, 28)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna_lookup = rnau.loc[ensids].to_numpy()\n",
    "rna_lookup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('RegressData/Regress_Feb16_data_AllLookups.npz',\n",
    "                   ml_lookup=ml_lookup,\n",
    "                   atac_lookup=atac_lookup,\n",
    "                   rna_lookup=rna_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ensid):\n",
    "    # For a list of ensids, return the x and y features\n",
    "    x = []\n",
    "    for i,ens in enumerate(ensid):\n",
    "        curr_enh = genes2enhu.loc[ens,['enh_pos','enh_num']]\n",
    "#         curr_enh = curr_enh[:1]\n",
    "#         E = curr_enh.groupby('ensid').count().max(axis=0);\n",
    "        mlu = df_mlevelu.loc[curr_enh['enh_pos']]\n",
    "        atacu = df_atacu.loc[curr_enh['enh_pos']]\n",
    "        Ne,Nc = mlu.shape\n",
    "        if use_kmers:\n",
    "            kmeru = kmer_countsu[curr_enh['enh_num']]\n",
    "            Ne,Nk = kmeru.shape\n",
    "            kmeru = kmeru[:,np.newaxis,:]\n",
    "            kmeru = np.broadcast_to(kmeru,(Ne,Nc,Nk))\n",
    "            x1 = np.dstack((mlu,atacu,kmeru))\n",
    "        else:\n",
    "            x1 = np.dstack((mlu,atacu))\n",
    "        x1 = x1[np.newaxis,:,:,:]\n",
    "        x.append(x1)\n",
    "    \n",
    "    # Pad the dimensions\n",
    "    max_nenh = np.max([xi.shape[1] for xi in x])\n",
    "    xpad = []\n",
    "    for xi in x:\n",
    "        xpad.append(np.pad(xi, ((0,0),(0,max_nenh-xi.shape[1]),(0,0),(0,0))))\n",
    "    x = np.concatenate(xpad, axis=0)\n",
    "\n",
    "    y = rnau.loc[ensid].to_numpy().reshape((-1,Nc))\n",
    "    \n",
    "    # Testing:\n",
    "    for c in range(Nc):\n",
    "        y[:,c] = y[:,c]*0+c+0.123\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "\n",
    "def get_data_index(ensid_index):\n",
    "    # For a list of ensids, return the x and y features\n",
    "    mlu = ml_lookup[ensid_index,:,:] # Ngenes x Ne x Nc\n",
    "    atacu = ml_lookup[ensid_index,:,:]\n",
    "    x = np.stack((mlu,atacu),axis=3)\n",
    "    y = rna_lookup[ensid_index,:]\n",
    "    \n",
    "#     # Testing:\n",
    "#     for c in range(Nc):\n",
    "#         y[:,c] = y[:,c]*0+c+0.123\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    return {'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = df_mlevelu.shape[1]\n",
    "\n",
    "# Define a class for the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2+K, N1);\n",
    "        self.fc2 = nn.Linear(3*N1, N2);\n",
    "        self.fc3 = nn.Linear(N2*Nc, Nc);\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.fc1(x)) # In: N x Eg x C x (2+K), Out: N x Eg x C x N1\n",
    "        xmax = torch.max(x, 1)[0]       # Out: N x C x N1\n",
    "        xmean = torch.mean(x, 1)       # Out: N x C x N1\n",
    "        xsum = torch.sum(x, 1)       # Out: N x C x N1\n",
    "        x = torch.cat((xmax,xmean,xsum),2) # Out: N x C x 3*N1\n",
    "        x = F.relu(self.fc2(x))       # Out: N x C x N2\n",
    "        x = torch.reshape(x,(-1,N2*Nc)) # Out: N x C\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "if device is not \"cpu\":\n",
    "    net.to(torch.device(device))\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(get_data(ensids[:3])['x'].to(device)).shape\n",
    "# get_data(ensids[:15])['x'].shape, get_data(ensids[:2])['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss=nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1)\n",
    "\n",
    "running_time = 0\n",
    "loss_vec = corr_vec = []\n",
    "nepochs = 0\n",
    "\n",
    "loss_test=np.array([])\n",
    "loss_train = np.array([])\n",
    "nparams = np.array([])\n",
    "l1params = np.array([])\n",
    "\n",
    "# Train/Test split\n",
    "test = [c in ['chr10','chr12','chr14'] for c in rnau.join(genes)['chr']];\n",
    "train = [not i for i in test]\n",
    "\n",
    "test = np.random.permutation(np.nonzero(test)[0]).squeeze()\n",
    "train = np.random.permutation(np.nonzero(train)[0]).squeeze()\n",
    "\n",
    "ensids = rnau.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%d-%m-%Y')\n",
    "fn_save = 'Regress_pytorch_N_%d_%d.%s.torch' % (N1,N2,today)\n",
    "fn_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "i = 10\n",
    "def testfn():\n",
    "    indices = train[i:i+batch_size]\n",
    "    print(indices)\n",
    "\n",
    "    # Input should be of size: (batch, channels, samples)\n",
    "#     batch_data = get_data(ensids[indices])\n",
    "    batch_data = get_data_index(indices)\n",
    "    batch_X = batch_data['x']\n",
    "    batch_y = batch_data['y']\n",
    "#         print(batch_X.shape, batch_y.shape)\n",
    "\n",
    "    # Send training data to CUDA\n",
    "    if device is not \"cpu\":\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(batch_X)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_data_index(indices)\n",
    "batch_X = batch_data['x'].to(device)\n",
    "batch_y = batch_data['y']\n",
    "net(batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f testfn testfn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "num_epochs = 1000;\n",
    "batch_size = 64;\n",
    "\n",
    "loss_train, loss_test = np.array([]), np.array([])\n",
    "t0 = time.time()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    nsamp = 0\n",
    "    running_loss = 0.0\n",
    "    running_time = 0.0\n",
    "    net.train()\n",
    "    t0train = time.time()\n",
    "    for i in range(0, len(train), batch_size):\n",
    "        tstart = time.time()\n",
    "        indices = train[i:i+batch_size]\n",
    "\n",
    "        # Input should be of size: (batch, channels, samples)\n",
    "        batch_data = get_data_index(indices)\n",
    "        batch_X = batch_data['x']\n",
    "        batch_y = batch_data['y']\n",
    "#         print(batch_X.shape, batch_y.shape)\n",
    "\n",
    "        # Send training data to CUDA\n",
    "        if device is not \"cpu\":\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if np.isnan(running_loss):\n",
    "            break\n",
    "        running_time += time.time()-tstart\n",
    "        nsamp += len(indices)\n",
    "        if (time.time()-t0train)>5:\n",
    "            print('Epoch %d, i=%d/%d, loss=%3.8f, t=%3.3f, %3.5f s/sample' % (epoch, i, len(train), \n",
    "                                                                              running_loss/nsamp, running_time, running_time/nsamp))\n",
    "            t0train=time.time()\n",
    "            print(outputs[0,:])\n",
    "\n",
    "    loss_train = np.append(loss_train,running_loss/nsamp)\n",
    "    scheduler.step(running_loss/nsamp)\n",
    "\n",
    "    net.eval()\n",
    "    running_loss_test = 0.0\n",
    "    nsamp = 0\n",
    "    for i in range(0, len(test), batch_size):\n",
    "        indices = test[i:i+batch_size]\n",
    "\n",
    "        # Input should be of size: (batch, channels, samples)\n",
    "        batch_data = get_data(ensids[indices])\n",
    "        batch_X = batch_data['x']\n",
    "        batch_y = batch_data['y']\n",
    "\n",
    "        # Send training data to CUDA\n",
    "        if device is not \"cpu\":\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        running_loss_test += loss.item()\n",
    "        nsamp += len(indices)\n",
    "    loss_test = np.append(loss_test,running_loss_test/nsamp)\n",
    "\n",
    "    if (time.time()-t0)>5:\n",
    "        plt.clf()\n",
    "        plt.semilogy(loss_train,'o-',label='Train')\n",
    "        plt.semilogy(loss_test,'o-',label='Test')\n",
    "        plt.legend()\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        print('**** Epoch %d, LR=%3.3f, loss_train=%3.8f, loss_test=%3.8f, time = %3.5f s/epoch' % (epoch, \n",
    "                                                                                                    optimizer.state_dict()['param_groups'][0]['lr'], \n",
    "                                                                                                    loss_train[-1], \n",
    "                                                                                                    loss_test[-1], \n",
    "                                                                                                    (time.time()-t0)/epoch))\n",
    "        print(outputs[0,:])\n",
    "        t0=time.time()\n",
    "\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': net.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss_train': loss_train,\n",
    "#             'loss_test': loss_test,\n",
    "#             }, fn_save)\n",
    "#     print('Saved data: %s' % fn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "indices = test\n",
    "\n",
    "yyhat = {'y':[], 'yhat':[]}\n",
    "for ensid in ensids[indices]:\n",
    "    # Input should be of size: (batch, channels, samples)\n",
    "    batch_data = get_data([ensid])\n",
    "    batch_X = batch_data['x']\n",
    "    batch_y = batch_data['y']\n",
    "\n",
    "    # Send training data to CUDA\n",
    "    if device is not \"cpu\":\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "    outputs = net(batch_X)\n",
    "    yyhat['yhat'].append(outputs.detach().cpu().numpy())\n",
    "    yyhat['y'].append(batch_y)\n",
    "yyhat['yhat'] = np.stack(yyhat['yhat']).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yyhat['yhat'].T,'.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ensids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce088403e063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mensids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ensids' is not defined"
     ]
    }
   ],
   "source": [
    "ensids[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
