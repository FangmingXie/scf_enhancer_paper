{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Train a regression to predict the mCG level of all DMRs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import datetime, random, glob, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kmers = True\n",
    "N1 = 64\n",
    "N2 = 32\n",
    "N3 = 16\n",
    "lr = 0.002\n",
    "batch_size = 2 if use_kmers else 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RegressEnh0004_UseKmers_N_64_32_16.25-02-2020.pt', 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today=datetime.datetime.today().strftime('%d-%m-%Y')\n",
    "fn_id = len(glob.glob('./RegressEnh*.pt'))+1 # Generate a unique ID for this run\n",
    "fn_save = 'RegressEnh%0.4d_%s_N_%d_%d_%d.%s.pt' % (fn_id, ('UseKmers' if use_kmers else 'NoKmers'), N1,N2,N3,today)\n",
    "fn_save, fn_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RegressData/Regress_Feb16_data_rnau.pkl\n",
      "Loaded RegressData/Regress_Feb16_data_genes.pkl\n",
      "(62698, 2080)\n",
      "Loaded kmers\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "fn_load_prefix = 'RegressData/Regress_Feb16_data_'\n",
    "# save_vars = ['genes2enhu', 'rnau', 'df_mlevelu', 'df_atacu', 'genes']\n",
    "save_vars = ['rnau','genes']\n",
    "for var in save_vars:\n",
    "    fn = fn_load_prefix+var+'.pkl'\n",
    "    cmd = '%s=pd.read_pickle(\"%s\")' % (var, fn)\n",
    "    exec(cmd)\n",
    "    print('Loaded %s' % fn)\n",
    "\n",
    "if use_kmers:\n",
    "    with np.load(fn_load_prefix+'kmer_countsu.npz', allow_pickle=True) as x:\n",
    "        kmer_countsu=x['kmer_countsu']\n",
    "    kmer_countsu = kmer_countsu/kmer_countsu.shape[1]/100\n",
    "    print(kmer_countsu.shape)\n",
    "    Nk=kmer_countsu.shape[1]\n",
    "    print('Loaded kmers')\n",
    "else:\n",
    "    Nk=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove most of the kmers\n",
    "kmer_countsu = kmer_countsu[:,:1]\n",
    "Nk=kmer_countsu.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(fn_load_prefix+'AllLookups.npz',allow_pickle=True) as data:\n",
    "    ml_lookup=data['ml_lookup']\n",
    "    atac_lookup=data['atac_lookup']\n",
    "    rna_lookup=data['rna_lookup']\n",
    "    kmeri_lookup=data['kmeri_lookup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = ml_lookup.shape[2]\n",
    "def get_data_index(ensid_index):\n",
    "    # For a list of ensids, return the x and y features\n",
    "    Ng = len(ensid_index)\n",
    "    mlu = ml_lookup[ensid_index,:,:] # Ngenes x Ne x Nc\n",
    "    atacu = atac_lookup[ensid_index,:,:]\n",
    "    if use_kmers:\n",
    "        nz = np.max([len(kmeri_lookup[x]) for x in ensid_index])\n",
    "        kmeru = np.zeros((Ng,nz,1,Nk))\n",
    "        for i,ens in enumerate(ensid_index):\n",
    "            kmeru[i,:len(kmeri_lookup[ens]),0,:] = kmer_countsu[kmeri_lookup[ens],:]\n",
    "        kmeru = np.broadcast_to(kmeru, (Ng,nz,Nc,Nk))\n",
    "    else:\n",
    "        nz = np.sum((np.sum(atacu>0,axis=(0,2))+np.sum(mlu>0,axis=(0,2)))>0)\n",
    "        kmeru = None\n",
    "\n",
    "    # Keep only the enhancers that have data\n",
    "    mlu = mlu[:,:nz,:,np.newaxis]\n",
    "    atacu = atacu[:,:nz,:,np.newaxis]\n",
    "\n",
    "    epi = np.concatenate((mlu,atacu),axis=3)\n",
    "    y = rna_lookup[ensid_index,:]\n",
    "\n",
    "#     # Test: add y to the input features\n",
    "#     x[:,:,:,0] = y[:,np.newaxis,:]\n",
    "    \n",
    "#     # Testing:\n",
    "#     for c in range(Nc):\n",
    "#         y[:,c] = y[:,c]*0+c+0.123\n",
    "    \n",
    "    epi = torch.tensor(epi, dtype=torch.float)\n",
    "    kmeru = torch.tensor(kmeru, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    return {'epi': epi, 'kmers': kmeru, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = ml_lookup.shape[2]\n",
    "\n",
    "# Define a class for the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1_kmers = nn.Linear(Nk, N1);\n",
    "        self.fc1_epi = nn.Linear(2, N1);\n",
    "        self.fc2 = nn.Linear(3*N1, N2);\n",
    "        self.fc3 = nn.Linear(N2, N3);\n",
    "        self.fc4 = nn.Linear(N3*Nc, Nc);\n",
    "        \n",
    "    def forward(self, epi, kmers):\n",
    "        if kmers.shape[0]>1:\n",
    "            x = F.relu(self.fc1_kmers(kmers) + self.fc1_epi(epi)) # In: N x Eg x C x (2+K), Out: N x Eg x C x N1\n",
    "        else:\n",
    "            x = F.relu(self.fc1_epi(epi))\n",
    "\n",
    "        # Collapse across enhancers in different ways\n",
    "        xmax = torch.max(x, 1)[0]       # Out: N x C x N1\n",
    "        xmean = torch.mean(x, 1)       # Out: N x C x N1\n",
    "        xsum = torch.sum(x, 1)/10000       # Out: N x C x N1\n",
    "        x = torch.cat((xmax,xmean,xsum),2) # Out: N x C x 3*N1\n",
    "\n",
    "        x = F.relu(self.fc2(x))       # Out: N x C x N2\n",
    "        x = F.relu(self.fc3(x))       # Out: N x C x N3\n",
    "        x = torch.reshape(x,(-1,N3*Nc)) # Out: N x C\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epi', 'kmers', 'y'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = get_data_index([2,3])\n",
    "batch_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "def myinit():\n",
    "    global net, optimizer, criterion, scheduler, loss_test, loss_train, test, train, ensids\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "\n",
    "    # Initialize the kmer weights to 0 and turn off learning\n",
    "    net.fc1_kmers.requires_grad_(False)\n",
    "    net.fc1_kmers.weight.fill_(0)\n",
    "    net.fc1_kmers.bias.fill_(0)\n",
    "    \n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25)\n",
    "    \n",
    "    loss_test=np.array([])\n",
    "    loss_train = np.array([])\n",
    "\n",
    "    # Train/Test split\n",
    "    test = [c in ['chr10','chr12','chr14'] for c in rnau.join(genes)['chr']];\n",
    "    train = [not i for i in test]\n",
    "\n",
    "    test = np.random.permutation(np.nonzero(test)[0]).squeeze()\n",
    "    train = np.random.permutation(np.nonzero(train)[0]).squeeze()\n",
    "    ensids = rnau.index.values\n",
    "    return\n",
    "\n",
    "def train_epoch(epoch, phase=2):\n",
    "    nsamp = 0\n",
    "    running_loss = 0.0\n",
    "    running_time = 0.0\n",
    "    net.train()\n",
    "    t0train = time.time()\n",
    "    for i in range(0, len(train), batch_size):\n",
    "        tstart = time.time()\n",
    "        indices = train[i:i+batch_size]\n",
    "\n",
    "        # Input should be of size: (batch, channels, samples)\n",
    "        batch_data = get_data_index(indices)\n",
    "        batch_epi = batch_data['epi']\n",
    "        batch_kmers = batch_data['kmers'] if phase==2 else torch.tensor([0.0])\n",
    "        batch_y = batch_data['y']\n",
    "\n",
    "        # Send training data to CUDA\n",
    "        if device is not \"cpu\":\n",
    "            batch_epi = batch_epi.to(device)\n",
    "            batch_kmers = batch_kmers.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(batch_epi, batch_kmers)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_time += time.time()-tstart\n",
    "        nsamp += len(indices)\n",
    "        if (time.time()-t0train)>5:\n",
    "            print('Epoch %d, i=%d/%d, LR=%3.5g, loss=%3.8f, t=%3.3f, %3.5f s/sample' % (epoch, i, len(train), \n",
    "                                                                                        optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "                                                                                        running_loss/nsamp, running_time, running_time/nsamp))\n",
    "            t0train=time.time()\n",
    "\n",
    "    return running_loss/nsamp\n",
    "\n",
    "def test_epoch(epoch, phase=2):\n",
    "    \n",
    "    net.eval()\n",
    "    running_loss_test = 0.0\n",
    "    nsamp = 0\n",
    "    yyhat = {'y':[], 'yhat':[]}\n",
    "    for i in range(0, len(test), batch_size):\n",
    "        indices = test[i:i+batch_size]\n",
    "\n",
    "        # Input should be of size: (batch, channels, samples)\n",
    "        batch_data = get_data_index(indices)\n",
    "        batch_epi = batch_data['epi']\n",
    "        batch_kmers = batch_data['kmers'] if phase==2 else torch.tensor([0.0])\n",
    "        batch_y = batch_data['y']\n",
    "\n",
    "        # Send training data to CUDA\n",
    "        if device is not \"cpu\":\n",
    "            batch_epi = batch_epi.to(device)\n",
    "            batch_kmers = batch_kmers.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(batch_epi, batch_kmers)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        running_loss_test += loss.item()\n",
    "        nsamp += len(indices)\n",
    "\n",
    "        yyhat['yhat'].append(outputs.detach().cpu().numpy())\n",
    "        yyhat['y'].append(batch_y.detach().cpu().numpy())\n",
    "\n",
    "    return running_loss_test/nsamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "# batch_size = 2\n",
    "# %lprun -f get_data_index testfn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # %lprun -f get_data_index testfn()\n",
    "# for batch_size in 2**np.arange(0,10):\n",
    "#     print(batch_size)\n",
    "#     myinit()\n",
    "    \n",
    "#     net.train()\n",
    "#     %time testfn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(indices):\n",
    "    net.eval()\n",
    "    yyhat = {'y':[], 'yhat':[]}\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        i = indices[i:i+batch_size]\n",
    "\n",
    "        # Input should be of size: (batch, channels, samples)\n",
    "        batch_data = get_data_index(i)\n",
    "        batch_epi = batch_data['epi']\n",
    "        batch_kmers = batch_data['kmers']\n",
    "        batch_y = batch_data['y']\n",
    "\n",
    "        # Send training data to CUDA\n",
    "        if device is not \"cpu\":\n",
    "            batch_epi = batch_epi.to(device)\n",
    "            batch_kmers = batch_kmers.to(device)\n",
    "\n",
    "        outputs = net(batch_epi, batch_kmers)\n",
    "        \n",
    "        yyhat['yhat'].append(outputs.detach().cpu().numpy())\n",
    "        yyhat['y'].append(batch_y.numpy())\n",
    "    yyhat['yhat'] = np.concatenate(yyhat['yhat'],axis=0)\n",
    "    yyhat['y'] = np.concatenate(yyhat['y'],axis=0)\n",
    "    \n",
    "    cc = np.zeros((Nc,1))\n",
    "    for i in range(yyhat['y'].shape[1]):\n",
    "        cc[i,0] = np.corrcoef(yyhat['y'][:,i], yyhat['yhat'][:,i])[0,1]    \n",
    "\n",
    "    return yyhat, cc\n",
    "\n",
    "def make_plot1(save=False):\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.clf()\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.semilogx(loss_train[2:],'o-',label='Train')\n",
    "    plt.plot(loss_test[2:],'o-',label='Test')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(fn_save)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(yyhat_test['y'].T, yyhat_test['yhat'].T,'.');\n",
    "    plt.plot([0,3],[0,3],'k--')\n",
    "    plt.xlabel('True RNA expression')\n",
    "    plt.ylabel('Estimated RNA expression')\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(np.arange(Nc), cc)\n",
    "    plt.ylabel('Correlation coef.')\n",
    "    plt.xlabel('Cell type')\n",
    "    plt.legend(['Train','Test'])\n",
    "    if save:\n",
    "        fn_plot = 'Plots/'+fn_save.replace('.torch','')+'_corrcoef.png'\n",
    "        plt.savefig(fn_plot)\n",
    "        print('Saved plot: '+fn_plot)\n",
    "    plt.show();\n",
    "        \n",
    "def make_plot2(save=False):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i in range(Nc):\n",
    "        plt.subplot(5,6,i+1)\n",
    "        plt.plot([0,2],[0,2],'k--')\n",
    "        plt.plot(yyhat_train['y'][:,i], yyhat_train['yhat'][:,i],'.');\n",
    "        plt.plot(yyhat_test['y'][:,i], yyhat_test['yhat'][:,i],'.');\n",
    "    #     cc = np.corrcoef(yyhat['y'][:,i], yyhat['yhat'][:,i])[0,1]\n",
    "        plt.title('r=%3.3f train/%3.3f test' % (cc[i,0], cc[i,1]))\n",
    "    if save:\n",
    "        fn_plot='Plots/'+fn_save.replace('.torch','')+'_scatter.png'\n",
    "        plt.savefig(fn_plot)\n",
    "        print('Saved plot: '+fn_plot)\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a2cc89b197c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-4952000e55ef>\u001b[0m in \u001b[0;36mmyinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Initialize the kmer weights to 0 and turn off learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "myinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e2cd555e9125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# loop over the dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnew_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4952000e55ef>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, phase)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mt0train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs1, num_epochs2 = 50, 200;\n",
    "\n",
    "t0 = time.time()\n",
    "batch_size = 16\n",
    "for epoch in range(num_epochs1):  # loop over the dataset multiple times\n",
    "    new_loss_train = train_epoch(epoch, phase=1);\n",
    "    loss_train = np.append(loss_train, new_loss_train)\n",
    "    \n",
    "    new_loss_test = test_epoch(epoch, phase=1);\n",
    "    loss_test = np.append(loss_test,new_loss_test)\n",
    "    scheduler.step(new_loss_test)\n",
    "\n",
    "    print('**** Phase1 epoch %d, LR=%3.5g, loss_train=%3.8f, loss_test=%3.8f, time = %3.5f s/epoch' % (epoch, \n",
    "                    optimizer.param_groups[0]['lr'],\n",
    "                    loss_train[-1], \n",
    "                    loss_test[-1], \n",
    "                    (time.time()-t0))\n",
    "         )\n",
    "\n",
    "    if (time.time()-t0)>10 or (epoch==num_epochs1-1):        \n",
    "        if (epoch>0):            \n",
    "            cc = np.zeros((Nc,2))\n",
    "            yyhat_train, cc[:,[0]] = test_net(random.sample(train.tolist(), 500))\n",
    "            yyhat_test, cc[:,[1]] = test_net(test)\n",
    "            make_plot1(save=True)\n",
    "            make_plot2(save=True)  \n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        t0=time.time()\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_test': loss_test,\n",
    "                }, fn_save)\n",
    "        print('Saved data: %s' % fn_save)\n",
    "        \n",
    "# Turn on the learning for kmers\n",
    "net.fc1_kmers.requires_grad_(True)\n",
    "# Reset the learning rate for the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr) \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25)\n",
    "\n",
    "batch_size = 2\n",
    "for epoch in range(num_epochs2):  # loop over the dataset multiple times\n",
    "    new_loss_train = train_epoch(epoch);\n",
    "    loss_train = np.append(loss_train,new_loss_train)\n",
    "    \n",
    "    new_loss_test = test_epoch(epoch);\n",
    "    loss_train = np.append(loss_train,new_loss_train)\n",
    "    scheduler.step(new_loss_test)\n",
    "\n",
    "\n",
    "    print('**** Phase2 epoch %d, LR=%3.5g, loss_train=%3.8f, loss_test=%3.8f, time = %3.5f s/epoch' % (epoch, \n",
    "                    optimizer.param_groups[0]['lr'],\n",
    "                    loss_train[-1], \n",
    "                    loss_test[-1], \n",
    "                    (time.time()-t0))\n",
    "         )\n",
    "\n",
    "    if (time.time()-t0)>10 or (epoch==num_epochs1-1):        \n",
    "        if (epoch>1):            \n",
    "            cc = np.zeros((Nc,2))\n",
    "            yyhat_train, cc[:,[0]] = test_net(random.sample(train.tolist(), 500))\n",
    "            yyhat_test, cc[:,[1]] = test_net(test)\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            make_plot1(save=True)\n",
    "            make_plot2(save=True)  \n",
    "            t0=time.time()\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss_train': loss_train,\n",
    "                'loss_test': loss_test,\n",
    "                }, fn_save)\n",
    "        print('Saved data: %s' % fn_save)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2;\n",
    "cc = np.zeros((Nc,2))\n",
    "yyhat_train, cc[:,[0]] = test_net(random.sample(train.tolist(), 500))\n",
    "yyhat_test, cc[:,[1]] = test_net(test)\n",
    "make_plot1(save=True)\n",
    "make_plot2(save=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "# N = 64 x 32\n",
    "# **** Epoch 39, LR=1e-05, loss_train=0.00182151, loss_test=0.00199540, time = 0.15510 s/epoch\n",
    "# **** Epoch 0, LR=0.002, loss_train=6.51633886, loss_test=5.99391724, time = 19.92587 s/epoch\n",
    "\n",
    "# N = 32 x 8\n",
    "# **** Epoch 53, LR=1e-07, loss_train=0.00180459, loss_test=0.00203680, time = 0.12214 s/epoch\n",
    "# **** Epoch 71, LR=0.00025, loss_train=0.00172845, loss_test=0.00188889, time = 0.09076 s/epoch\n",
    "\n",
    "\n",
    "# N = 32 x 16\n",
    "# **** Epoch 85, LR=0.000, loss_train=0.00167278, loss_test=0.00183274, time = 0.07683 s/epoch\n",
    "# **** Epoch 65, LR=1e-06, loss_train=0.00168914, loss_test=0.00189639, time = 0.09208 s/epoch\n",
    "# **** Epoch 53, LR=5e-07, loss_train=0.00172672, loss_test=0.00198197, time = 0.10991 s/epoch\n",
    "# **** Epoch 58, LR=0.0005, loss_train=0.00167136, loss_test=0.00189098, time = 0.11401 s/epoch\n",
    "\n",
    "# N = 16 x 8\n",
    "# **** Epoch 53, LR=1e-05, loss_train=0.00177558, loss_test=0.00194236, time = 0.11628 s/epoch\n",
    "\n",
    "# N = 4 x 4\n",
    "# **** Epoch 66, LR=1.5625e-05, loss_train=0.00180230, loss_test=0.00197317, time = 0.09088 s/epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
